{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualise the top 10 largest number \n",
    "# Get the top 10 'daysincelastorder' values with the highest churn counts\n",
    "top_10 = grouped.sum(axis=1).nlargest(10)\n",
    "\n",
    "# Filter the original DataFrame for the top 10 'daysincelastorder' values\n",
    "df_top_10 = df[df['daysincelastorder'].isin(top_10.index)]\n",
    "\n",
    "# Create a pivot table to prepare data for a stacked bar plot\n",
    "pivot_table = df_top_10.pivot_table(index='daysincelastorder', columns='churn', aggfunc='size', fill_value=0)\n",
    "\n",
    "# Create a stacked bar plot\n",
    "ax = pivot_table.plot(kind='bar', stacked=True, figsize=(10, 6))\n",
    "plt.xlabel('Days Since Last Order')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Stacked Bar Plot of Churn vs. Days Since Last Order')\n",
    "plt.legend(title='Churn', labels=['0- Not Churned', '1- Churned'])\n",
    "\n",
    "# Annotate the values on the bars\n",
    "for index, row in pivot_table.iterrows():\n",
    "    plt.annotate(f'0: {row[0]}\\n1: {row[1]}', (index, row[0] + row[1] / 2), ha='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only churn (churn equals 1)\n",
    "churned_df = df[df['churn'] == 1]\n",
    "churned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'daysincelastorder' and calculate the count of churn values (churn equals 1)\n",
    "grouped = churned_df.groupby('daysincelastorder')['churn'].count()\n",
    "\n",
    "# Get the top 5 'daysincelastorder' values with the highest churn count\n",
    "top_5 = grouped.nlargest(5)\n",
    "\n",
    "print(top_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram using Plotly\n",
    "fig = px.bar(\n",
    "    x=top_5.index,\n",
    "    y=top_5,\n",
    "    title='Histogram of Churn Counts for Top 5 Days Since Last Order',\n",
    "    labels={'x': 'Days Since Last Order', 'y': 'Churn Count'},\n",
    ")\n",
    "# Display the chart\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "logistic_roc_auc = roc_auc_score(y_test, logistic_model.predict_proba(X_test)[:, 1])\n",
    "fpr, tpr, thresholds = roc_curve(y_test, logistic_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {logistic_roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # feature selection\n",
    "# why perform feature selection\n",
    "# Feature selection is the process of selecting the most important features for a machine learning model. \n",
    "# This can be done for a variety of reasons, such as to improve model performance, \n",
    "# reduce model complexity, and make the model more interpretable.\n",
    "\n",
    "# Information gain: This measure is used to assess how much information a feature provides about the target variable. \n",
    "#        Features with high information gain are typically more important for predicting the target variable.\n",
    "# Mutual information: This measure is similar to information gain, but it takes into account the nonlinear \n",
    "#       relationships between features. Features with high mutual information are typically more important for predicting the target variable.\n",
    "# Recursive feature elimination: This method works by iteratively removing the least important feature from a model \n",
    "#       until a stopping criterion is met. The stopping criterion may be based on the model performance, the number of features remaining, or some other measure.\n",
    "# LASSO and Ridge regression: These regularization methods shrink the coefficients of unimportant features to zero, \n",
    "#       effectively removing them from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Question: is it possible for precision, recall, accuracy to have thesame value\n",
    " \n",
    " Yes, it is possible for precision, recall, and accuracy to have the same value in a classification report, especially when you have a balanced dataset with equal numbers of true positives, true negatives, false positives, and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meaning of the followning evaluation metrics \n",
    "Evaluation metrics are used to assess the performance of machine learning models, particularly in classification tasks. Here are some commonly used evaluation metrics, along with their formulas and explanations:\n",
    "\n",
    "Accuracy:\n",
    "    Formula: (TP + TN) / (TP + TN + FP + FN)\n",
    "    Explanation: Accuracy measures the proportion of correctly predicted instances out of all instances. It is a straightforward metric but may not be suitable for imbalanced datasets.\n",
    "\n",
    "Precision (Positive Predictive Value):\n",
    "    Formula: TP / (TP + FP)\n",
    "    Explanation: Precision measures the proportion of true positive predictions among all positive predictions. It is valuable when minimizing false positives is critical.\n",
    "\n",
    "Recall (Sensitivity, True Positive Rate):\n",
    "    Formula: TP / (TP + FN)\n",
    "    Explanation: Recall measures the proportion of true positives among all actual positives. It is valuable when minimizing false negatives is critical.\n",
    "\n",
    "F1 Score:\n",
    "    Formula: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "    Explanation: The F1 score is the harmonic mean of precision and recall. It balances precision and recall and is useful when you want to find a compromise between the two.\n",
    "\n",
    "Specificity (True Negative Rate):\n",
    "    Formula: TN / (TN + FP)\n",
    "    Explanation: Specificity measures the proportion of true negatives among all actual negatives. It is useful when you want to evaluate a model's performance in correctly identifying negatives.\n",
    "\n",
    "ROC Curve (Receiver Operating Characteristic Curve):\n",
    "    Formula: ROC is not a single numeric metric but a graphical representation of the true positive rate (Recall) against the false positive rate at various thresholds.\n",
    "    Explanation: The ROC curve shows the trade-off between true positives and false positives at different classification thresholds. The area under the ROC curve (AUC-ROC) is often used as a metric, with higher values indicating better model performance.\n",
    "\n",
    "AUC-ROC (Area Under the ROC Curve):\n",
    "    Formula: Calculated as the area under the ROC curve.\n",
    "    Explanation: AUC-ROC summarizes the model's ability to distinguish between positive and negative classes. A higher AUC-ROC indicates better model performance.\n",
    "\n",
    "AUC-PR (Area Under the Precision-Recall Curve):\n",
    "    Formula: Calculated as the area under the precision-recall curve.\n",
    "    Explanation: AUC-PR summarizes the trade-off between precision and recall. It is useful for imbalanced datasets where the positive class is rare.\n",
    "\n",
    "Log Loss (Logarithmic Loss, Cross-Entropy Loss):\n",
    "    Formula: -Σ(y log(p) + (1 - y) log(1 - p)), where y is the actual class (0 or 1) and p is the predicted probability of the positive class.\n",
    "    Explanation: Log Loss measures the accuracy of a classifier's probability estimates. Lower log loss values indicate better-calibrated models.\n",
    "\n",
    "Mean Absolute Error (MAE):\n",
    "    Formula: (1/n) * Σ|actual - predicted|\n",
    "    Explanation: MAE measures the average absolute difference between actual and predicted values. It is commonly used in regression tasks.\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "    Formula: (1/n) * Σ(actual - predicted)^2\n",
    "    Explanation: MSE measures the average squared difference between actual and predicted values. It penalizes larger errors more than MAE and is also used in regression tasks.\n",
    "\n",
    "Root Mean Squared Error (RMSE):\n",
    "    Formula: √MSE\n",
    "    Explanation: RMSE is the square root of MSE. It's in the same units as the target variable and provides a measure of the average magnitude of errors. RMSE is more sensitive to outliers compared to MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploying model to cloud\n",
    "- create a web service: it is use to communicate between electronic devices."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
